{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/yilia/source/main.css","path":"main.css","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/default-skin.svg","path":"fonts/default-skin.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.eot","path":"fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.svg","path":"fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.ttf","path":"fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/tooltip.svg","path":"fonts/tooltip.svg","modified":0,"renderable":1},{"_id":"themes/yilia/source/fonts/iconfont.woff","path":"fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/yilia/source/main.js","path":"main.js","modified":0,"renderable":1},{"_id":"themes/yilia/source/assets/img/profile.jpg","path":"assets/img/profile.jpg","modified":0,"renderable":1}],"Cache":[{"_id":"themes/yilia/.editorconfig","hash":"daaa8757fac18f8735fadd0a37a42c06f421ca14","modified":1480666334596},{"_id":"themes/yilia/.gitignore","hash":"1685d565fc1096b2f21f6719e872a623c130f169","modified":1481037990766},{"_id":"themes/yilia/README.md","hash":"4751bbbfe4ea5f2acd3d864de893e19c5a3d6878","modified":1480666334596},{"_id":"themes/yilia/_config.yml","hash":"97e2c3777c79124a9469af5a9254f8d03ebb26e9","modified":1481038497869},{"_id":"themes/yilia/package.json","hash":"3133f9792e28ae20a16f27fdec682e051dbf21c0","modified":1480666334642},{"_id":"themes/yilia/webpack.config.js","hash":"1cc836dc29277fd211233849867fd58bc64a0d3b","modified":1480666334674},{"_id":"source/_posts/.project","hash":"96b0fcfbb5983eee8dc193c771627dae3c1e637d","modified":1481037990765},{"_id":"source/_posts/hexo-guide.md","hash":"ab33c4f3a10ec7950aa33eb1cb1b21a44e0bd6af","modified":1480673864745},{"_id":"source/_posts/hello-world-jaylucien.md","hash":"956985cbe5209147d0a7b251d24e4ef655033615","modified":1481037990766},{"_id":"source/_posts/standford-image-cnn-lesson-1.md","hash":"52037d9cffa7c3d2960c22743351940908334b46","modified":1481100526717},{"_id":"themes/yilia/languages/default.yml","hash":"f26a34a7983d4bc17c65c7f0f14da598e62ce66d","modified":1480666334596},{"_id":"themes/yilia/languages/fr.yml","hash":"b4be1c1592a72012e48df2b3ec41cc9685573e50","modified":1480666334596},{"_id":"themes/yilia/languages/nl.yml","hash":"3d82ec703d0b3287739d7cb4750a715ae83bfcb3","modified":1480666334596},{"_id":"themes/yilia/languages/no.yml","hash":"ddf2035e920a5ecb9076138c184257d9f51896a7","modified":1480666334596},{"_id":"themes/yilia/languages/ru.yml","hash":"2a476b4c6e04900914c81378941640ac5d58a1f0","modified":1480666334611},{"_id":"themes/yilia/languages/zh-CN.yml","hash":"b057f389c6713010f97d461e48ec959b0b6f3b44","modified":1480666334611},{"_id":"themes/yilia/languages/zh-tw.yml","hash":"f5f0ca88185da7a8457760d84bf221781473bd7c","modified":1480666334611},{"_id":"themes/yilia/layout/.project","hash":"2c012c2be9087bb28002196d53731066a4f91e6b","modified":1481037990766},{"_id":"themes/yilia/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1480666334627},{"_id":"themes/yilia/layout/category.ejs","hash":"765426a9c8236828dc34759e604cc2c52292835a","modified":1480666334627},{"_id":"themes/yilia/layout/index.ejs","hash":"ec498c6c0606acde997ce195dad97b267418d980","modified":1480666334627},{"_id":"themes/yilia/layout/layout.ejs","hash":"6759bdc3646d6c03f9dbc7abffcb2b5e4522c724","modified":1480666334627},{"_id":"themes/yilia/layout/page.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1480666334627},{"_id":"themes/yilia/layout/post.ejs","hash":"7d80e4e36b14d30a7cd2ac1f61376d9ebf264e8b","modified":1480666334642},{"_id":"themes/yilia/layout/tag.ejs","hash":"eaa7b4ccb2ca7befb90142e4e68995fb1ea68b2e","modified":1480666334642},{"_id":"themes/yilia/source/main.css","hash":"cee78af2f34e5b8572aec058c8db4b6fdc88d6d5","modified":1480666334674},{"_id":"themes/yilia/layout/_partial/after-footer.ejs","hash":"327978b8d1d34885bac46edef59e5e0883a8d324","modified":1480666334611},{"_id":"themes/yilia/layout/_partial/archive.ejs","hash":"a4eacc2bc1278095a0ef99f904b0634c78f980eb","modified":1480666334611},{"_id":"themes/yilia/layout/_partial/archive-post.ejs","hash":"edc0154b30a4127acda10297bec6aacf754b4ac4","modified":1480666334611},{"_id":"themes/yilia/layout/_partial/article.ejs","hash":"c3cb4160b3f9cd817f2d829807ed9d3488f345ed","modified":1480666334611},{"_id":"themes/yilia/layout/_partial/google-analytics.ejs","hash":"1ccc627d7697e68fddc367c73ac09920457e5b35","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/footer.ejs","hash":"871f81cacd5d41cb2eb001cd56254217a857dc2f","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/head.ejs","hash":"0d5f8d8f4c1aefcf34f816cb8ebda41a8926b547","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/header.ejs","hash":"63d53c26f6ef7d2b4d96de3a2d3d7bd385f8dfda","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/left-col.ejs","hash":"da4ce0c2e3c15ff5ebccbfe77404d1a3773ded03","modified":1481038295950},{"_id":"themes/yilia/layout/_partial/mathjax.ejs","hash":"11550a418921d330e6553be0569a94ab5a217967","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/mobile-nav.ejs","hash":"d263cd48b0f246872a2bd8f192ffb839805f27bc","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/tools.ejs","hash":"fe0fd2f17b14d3fedfef43b6acadead38183b2a2","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/viewer.ejs","hash":"cc1c39903aed0a0601d104238d2bbd13ad2a36f3","modified":1480666334627},{"_id":"themes/yilia/source/fonts/default-skin.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1480666334674},{"_id":"themes/yilia/source/fonts/iconfont.eot","hash":"bc7decf4e37c3df6bd81d617d951f83327faa742","modified":1480666334674},{"_id":"themes/yilia/source/fonts/iconfont.svg","hash":"2a3f2f4773f2af59d4ef37b13aed39a5bcd7e018","modified":1480666334674},{"_id":"themes/yilia/source/fonts/iconfont.ttf","hash":"d1d9497f08d75c36af6b1a4e5ee7a82e912da18e","modified":1480666334674},{"_id":"themes/yilia/source/fonts/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1480666334674},{"_id":"themes/yilia/source/fonts/iconfont.woff","hash":"27523a9a8009e1599f6bb84e456d4b6506e62dd3","modified":1480666334674},{"_id":"themes/yilia/source/main.js","hash":"3ec824e855fe1346e90443900302a9b1bffcdaa4","modified":1480666334674},{"_id":"themes/yilia/source-src/css/_core.scss","hash":"29ba600e98ed55f7af4ade8038272c84cba21188","modified":1480666334642},{"_id":"themes/yilia/source-src/css/_function.scss","hash":"ce227b6f5a9af194fd5d455200630f32c05e151f","modified":1480666334642},{"_id":"themes/yilia/source-src/css/archive-inner.scss","hash":"9503a730c0d7a9d8f9fdc81cd32b474a6d94ec57","modified":1480666334642},{"_id":"themes/yilia/source-src/css/archive.scss","hash":"8a0ae8ee6af8df3f215f1cd4ecc10145a5b92cf0","modified":1480666334642},{"_id":"themes/yilia/source-src/css/article-inner.scss","hash":"4115c4518cca3f5cf461018aa7a7321138d017cc","modified":1480666334642},{"_id":"themes/yilia/source-src/css/article-main.scss","hash":"7802304b0f2d302f0652236e35a5096003ebd0d1","modified":1480666334642},{"_id":"themes/yilia/source-src/css/article-nav.scss","hash":"9069371dcc65eee63081a21e8a56a9f3dbf64c08","modified":1480666334642},{"_id":"themes/yilia/source-src/css/article.scss","hash":"133cd0a816cd4406415c2cf1f042f9039e65c805","modified":1480666334642},{"_id":"themes/yilia/source-src/css/duoshuo.scss","hash":"8eb9678e2f88da64436d75e5fde5c84bd52f7e17","modified":1480666334642},{"_id":"themes/yilia/source-src/css/fonts.scss","hash":"33ca561397b9a7e6ddfb7101959a0192a8431b72","modified":1480666334642},{"_id":"themes/yilia/source-src/css/global.scss","hash":"b4cb4f45a55d4250cd9056f76dab2a3c0dabcec4","modified":1480666334642},{"_id":"themes/yilia/source-src/css/footer.scss","hash":"7ca837a4cc34db1c35f01baec85eb10ccc64ea86","modified":1480666334642},{"_id":"themes/yilia/source-src/css/grid.scss","hash":"b8629b441b7bb1b1f4dcaeaf77df0dae5d101dad","modified":1480666334642},{"_id":"themes/yilia/source-src/css/highlight.scss","hash":"0267e2febaef284cf319235435643232fdeee0c9","modified":1480666334642},{"_id":"themes/yilia/source-src/css/main.scss","hash":"2534ff49f57b0cab095a538781c6ea3369199bc6","modified":1480666334658},{"_id":"themes/yilia/source-src/css/mobile-slider.scss","hash":"facd3d41bc9b2d2c6134fc0fbd379a3b0c18476b","modified":1480666334658},{"_id":"themes/yilia/source-src/css/left.scss","hash":"0940b74a29101c97573efd8e492ee39cef5e2dc8","modified":1480666334658},{"_id":"themes/yilia/source-src/css/mobile.scss","hash":"d5ead4f0787e72dea76a1e2bc114edc82e744451","modified":1480666334658},{"_id":"themes/yilia/source-src/css/page.scss","hash":"244c4d75c375978ff9edb74acc68825e63c6b235","modified":1480666334658},{"_id":"themes/yilia/source-src/css/reward.scss","hash":"a557a9ed244c82b8b71e9da9de3339d92783499f","modified":1480666334658},{"_id":"themes/yilia/source-src/css/scroll.scss","hash":"2495f7e4e3b055735c531f944b5f40a118a351ec","modified":1480666334658},{"_id":"themes/yilia/source-src/css/social.scss","hash":"920e6617c5053f09026ee512b3751f284698bf45","modified":1480666334658},{"_id":"themes/yilia/source-src/css/share.scss","hash":"c165c65bb82e1f736b9a87b5ca86c8aacb4d66ab","modified":1480666334658},{"_id":"themes/yilia/source-src/css/switch.scss","hash":"a4c1872aff9509441f4d723801a9b3f7db161a41","modified":1480666334658},{"_id":"themes/yilia/source-src/css/tags.scss","hash":"b109bf8726067b612ff38117f7541d1ea34b97bf","modified":1480666334658},{"_id":"themes/yilia/source-src/css/tags-cloud.scss","hash":"399744e98e7c67939ed9b23c2670d8baad044eda","modified":1480666334658},{"_id":"themes/yilia/source-src/css/tools.scss","hash":"86108d02e99a9c9fb3a8d8a9ac29599a8eeebde2","modified":1480666334658},{"_id":"themes/yilia/source-src/css/tooltip.scss","hash":"b81cedbe31accca82e597801186911a7b5e6841c","modified":1480666334658},{"_id":"themes/yilia/source-src/photoSwipe/photoswipe-ui-default.js","hash":"05fa305ec449deb59b04e2ae118a8e3ec5250e1b","modified":1480666334674},{"_id":"themes/yilia/source-src/photoSwipe/photoswipe-ui-default.min.js","hash":"852a1fcdaacf66754090fc6d432013c5c657ab80","modified":1480666334674},{"_id":"themes/yilia/source-src/photoSwipe/photoswipe.scss","hash":"b80bb4efe9ac36a566f037fb6984af8b486a9d5c","modified":1480666334674},{"_id":"themes/yilia/source-src/photoSwipe/photoswipe.min.js","hash":"39806b9989eaecbc3e032da8de77f69e0c9ff779","modified":1480666334674},{"_id":"themes/yilia/source-src/js/archive-inner.js","hash":"b7639058dc1043ec1a8c87f46b7d1acccac57105","modified":1480666334658},{"_id":"themes/yilia/source-src/js/browser.js","hash":"bb2e9a1ddcb34372e6cbdd9e9ecfd3dc87623451","modified":1480666334658},{"_id":"themes/yilia/source-src/js/fix-page.js","hash":"940b580d6243461788c28bdd09b8049642a90e13","modified":1480666334658},{"_id":"themes/yilia/source-src/js/jquery.lazyload.js","hash":"2258765aa8b606cc095900daedf2666c46383de8","modified":1480666334658},{"_id":"themes/yilia/source-src/js/main.js","hash":"f68b906d3abfae2ae6e740e6d10492d32fb276dd","modified":1480666334658},{"_id":"themes/yilia/source-src/js/mobile.js","hash":"d5abbdaf16fd4c8ee9ee802cfd4b6aeb34073893","modified":1480666334658},{"_id":"themes/yilia/source-src/js/qrcode.js","hash":"4b1c4a83543eb3aa0a4485db5594e5b634a47a38","modified":1480666334658},{"_id":"themes/yilia/source-src/js/report.js","hash":"57680f9a23bd0a1eaafd64ae08cc33e20627ab15","modified":1480666334658},{"_id":"themes/yilia/source-src/js/share.js","hash":"0973e1cc84f80b4a1dd31bd41cf0e9ebaf1cb15d","modified":1480666334674},{"_id":"themes/yilia/source-src/js/tags.js","hash":"a8da9d6e3031c960f9859ef519233ad3ab65d0ef","modified":1480666334674},{"_id":"themes/yilia/source-src/js/tools.js","hash":"eb36f563b76e9431dc109856a78383ed11763a84","modified":1480666334674},{"_id":"themes/yilia/source-src/js/util.js","hash":"cf91a9c9c973e2ee05d50a24a11c411c36264238","modified":1480666334674},{"_id":"themes/yilia/source-src/js/viewer.js","hash":"df39fafb3639dbcffbf80503f6e9bfee011ba328","modified":1480666334674},{"_id":"themes/yilia/source-src/photoSwipe/photoswipe.js","hash":"b616337e586eaa5afcf5bb77c927bd1b09a25524","modified":1480666334674},{"_id":"themes/yilia/layout/_partial/post/duoshuo.ejs","hash":"f6b4c4eaafb5ac386273354b5f64a26139b7a3b0","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/post/category.ejs","hash":"24f8f7fe3836f4852fdeecb0b844f3c14fd746e8","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/post/date.ejs","hash":"aae96de18d48cd3b9b7bf6fed0100e15b53cca97","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/post/nav.ejs","hash":"b6a97043f9ec37e571aacacfedcda1d4d75e3c7c","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/post/tag.ejs","hash":"e6edf173da77fb851fc067a481a897ad934cc4ca","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/post/title.ejs","hash":"d4a460a35e2112d0c7414fd5e19b3a16093f1caf","modified":1480666334627},{"_id":"themes/yilia/layout/_partial/post/share.ejs","hash":"0f768c01b7aae94e6daf0a8d8c2186592c27be15","modified":1480666334627},{"_id":"themes/yilia/source/assets/img/profile.jpg","hash":"76f956e57a51611e677f89aa11752533d7eb6a49","modified":1481037990768},{"_id":"themes/yilia/source-src/css/core/_animation.scss","hash":"8381a373d85daee53cc3247467ffa4db58ae1b88","modified":1480666334642},{"_id":"themes/yilia/source-src/css/core/_media-queries.scss","hash":"262ffcd88775080b7f511db37f58d2bcb1b2bfc7","modified":1480666334642},{"_id":"themes/yilia/source-src/css/core/_mixin.scss","hash":"4d2d5331a206768681add5e779789c58e9835f94","modified":1480666334642},{"_id":"themes/yilia/source-src/css/core/_variables.scss","hash":"6e75bdaa46de83094ba0873099c6e7d656a22453","modified":1480666334642},{"_id":"themes/yilia/source-src/css/core/_reset.scss","hash":"398a49913b4a47d928103562b1ce94520be4026a","modified":1480666334642},{"_id":"themes/yilia/source-src/css/fonts/iconfont.eot","hash":"bc7decf4e37c3df6bd81d617d951f83327faa742","modified":1480666334642},{"_id":"themes/yilia/source-src/css/fonts/iconfont.svg","hash":"2a3f2f4773f2af59d4ef37b13aed39a5bcd7e018","modified":1480666334642},{"_id":"themes/yilia/source-src/css/fonts/iconfont.ttf","hash":"d1d9497f08d75c36af6b1a4e5ee7a82e912da18e","modified":1480666334642},{"_id":"themes/yilia/source-src/css/fonts/iconfont.woff","hash":"27523a9a8009e1599f6bb84e456d4b6506e62dd3","modified":1480666334642},{"_id":"themes/yilia/source-src/css/img/checkered-pattern.png","hash":"049262fa0886989d750637b264bed34ab51c23c8","modified":1480666334642},{"_id":"themes/yilia/source-src/css/img/black-scales.png","hash":"243ea748d016704922ccfc0b6c18d97472c27bff","modified":1480666334642},{"_id":"themes/yilia/source-src/css/img/coderwall.png","hash":"fa84676c4d654e040e51fd34bfcd9f9348cd5331","modified":1480666334642},{"_id":"themes/yilia/source-src/css/img/douban.png","hash":"e2ade003ffadd5826ee66ec23901c2d6e8607e4e","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/github.png","hash":"b84d03b32fa388dcbf149296ebd16dce6223d48d","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/facebook.png","hash":"d19ad7a0903daf26817afd8753cd97e0cc714f54","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/google.png","hash":"61a21fec7346fa3400b747ac9a201cf3d5bc013d","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/mail.png","hash":"fca8199cc77fdbd700a45bf56d091c82f4a67fe7","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/linkedin.png","hash":"e203138fb53c257cb214e97f4e30091b9c568d2c","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/pinboard.png","hash":"0891fbb6d092fa012bf936019923383d84c6aeb0","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/pinterest.png","hash":"9c72917f8779c083157c6ce7a5d62ed4874f0630","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/rss.png","hash":"430fd47340e75214c081abd05cd7410cf7c71b86","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/scrollbar_arrow.png","hash":"d64a33c4ddfbdb89deeb6f4e3d36eb84dc4777c0","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/stackoverflow.png","hash":"da5dfe9043055c95e479d49c78cd3b020de608f2","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/twitter.png","hash":"14dbb8e62d056525253bc0de13acd1723da7a934","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/weibo.png","hash":"280dae3fd38086158b4a1b57edb94c06b1a5014b","modified":1480666334658},{"_id":"themes/yilia/source-src/css/img/zhihu.png","hash":"a6d6ef65e9ac82e613a311810391ebb90d9b1c1d","modified":1480666334658},{"_id":"themes/yilia/source-src/photoSwipe/default-skin/default-skin.png","hash":"ed95a8e40a2c3478c5915376acb8e5f33677f24d","modified":1480666334674},{"_id":"themes/yilia/source-src/photoSwipe/default-skin/default-skin.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1480666334674},{"_id":"themes/yilia/source-src/photoSwipe/default-skin/default-skin.scss","hash":"3f8f8062d24cce2158d3c02bdfc56c000f1a1f9b","modified":1480666334674},{"_id":"themes/yilia/source-src/photoSwipe/default-skin/preloader.gif","hash":"6342367c93c82da1b9c620e97c84a389cc43d96d","modified":1480666334674},{"_id":"themes/yilia/source-src/css/img/black-paper.png","hash":"a180d3109a5cb6b9b9aa60d81730446ebe275473","modified":1480666334642},{"_id":"themes/yilia/source-src/js/jquery.js","hash":"aa6ccf721c4e76921abda46c120772d364e5b285","modified":1480666334658},{"_id":"public/2016/12/06/hello-world-jaylucien/index.html","hash":"1d053835244255af236c6355a77844274d8f2cc7","modified":1481100582239},{"_id":"public/2016/12/02/hexo-guide/index.html","hash":"11fcb03c1ea64bc7170596a7c0391fcf54db22e6","modified":1481100582239},{"_id":"public/archives/index.html","hash":"5d554fbd64faf28dcdec2f74ad6d2762209cb5a7","modified":1481100582239},{"_id":"public/archives/2016/index.html","hash":"4716126cd785406aedafdb6b6816433770d830c2","modified":1481100582239},{"_id":"public/archives/2016/12/index.html","hash":"8e6359ff790a8a75b1386ca22fc55a41c0fdc196","modified":1481100582239},{"_id":"public/index.html","hash":"4bd7c9208350dc41602beea85925ac23f8b3880c","modified":1481100582239},{"_id":"public/2016/12/06/standford-image-cnn-lesson-1/index.html","hash":"7f7e1b7ff2669249071d293043507dc004590582","modified":1481100582239},{"_id":"public/fonts/default-skin.svg","hash":"2ac727c9e092331d35cce95af209ccfac6d4c7c7","modified":1481100582243},{"_id":"public/fonts/iconfont.eot","hash":"bc7decf4e37c3df6bd81d617d951f83327faa742","modified":1481100582243},{"_id":"public/fonts/iconfont.svg","hash":"2a3f2f4773f2af59d4ef37b13aed39a5bcd7e018","modified":1481100582243},{"_id":"public/fonts/iconfont.ttf","hash":"d1d9497f08d75c36af6b1a4e5ee7a82e912da18e","modified":1481100582243},{"_id":"public/fonts/tooltip.svg","hash":"397fe4b1093bf9b62457dac48aa15dac06b54a3c","modified":1481100582243},{"_id":"public/fonts/iconfont.woff","hash":"27523a9a8009e1599f6bb84e456d4b6506e62dd3","modified":1481100582243},{"_id":"public/assets/img/profile.jpg","hash":"76f956e57a51611e677f89aa11752533d7eb6a49","modified":1481100582244},{"_id":"public/main.css","hash":"cee78af2f34e5b8572aec058c8db4b6fdc88d6d5","modified":1481100582250},{"_id":"public/main.js","hash":"9b65612638edd96ffed8d3e3819f7bae491be9ed","modified":1481100582250}],"Category":[],"Data":[],"Page":[],"Post":[{"title":"hexo博客搭建过程记录","_content":"###前期准备\n####1. 安装NodeJs\n（未完待续）","source":"_posts/hexo-guide.md","raw":"---\ntitle: hexo博客搭建过程记录\n---\n###前期准备\n####1. 安装NodeJs\n（未完待续）","slug":"hexo-guide","published":1,"date":"2016-12-02T10:17:03.381Z","updated":"2016-12-02T10:17:44.745Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwep1n2w0000qsfjvi5hp3gt","content":"<p>###前期准备</p>\n<p>####1. 安装NodeJs<br>（未完待续）</p>\n","excerpt":"","more":"<p>###前期准备</p>\n<p>####1. 安装NodeJs<br>（未完待续）</p>\n"},{"title":"Hello World","_content":"初来乍到\n\n","source":"_posts/hello-world-jaylucien.md","raw":"---\ntitle: Hello World\n---\n初来乍到\n\n","slug":"hello-world-jaylucien","published":1,"date":"2016-12-06T15:26:30.765Z","updated":"2016-12-06T15:26:30.766Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwep1n310001qsfjkzq2kfxm","content":"<p>初来乍到</p>\n","excerpt":"","more":"<p>初来乍到</p>\n"},{"title":"斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法","_content":"<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=429482207&auto=0&height=66\"></iframe>\n\n### 斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法\n\n#### 图像分类\n**概念** \n将图像作为输入，然后从固定的分类集合中选择一个标签为其贴上，一般而言，一些突出的计算机视觉任务，例如物体检测、图像分割都可以转化成为图像分类任务。\n\n<!--more-->\n\n**举个栗子** \n现在我有一幅图片，然后准备了4中分类标签：猫、狗、帽子和杯子。如下图所示，这张带有猫的图片尺寸是400x248，带有RGB三通道，每个通道位宽是8位，所以这幅图片的原生数据个数是：400x248x3=297600，每个数据范围是0-255，图像分类的任务便是把这么多的数据输入，然后最终输出“猫”这个分类。\n\n![All text](http://7xopvd.com1.z0.glb.clouddn.com/classify.png)\n\n**挑战**\n虽然在这样一幅图中寻找一只猫这样的任务对人类而言很简单，但在技术那几视觉算法领域，其中包含的一些挑战和困难是值得去考虑的。下面罗列了一些存在的问题，需要记住的是，原生图像的表示是三维的，即RGB：\n- **视角变化** 同样的物体，由于相机位置的不同，造成了物体在图像中的方位不同。\n- **伸缩变化** 物体在图像中的尺寸可能和现实世界中分类好的物体尺寸不同\n- **形变** 许多物体的形状并不是刚性的，可能会有很极端的形变方式\n- **光照条件** 光照强度在像素级上的影响是很严重的，例如光照太强引起的过饱和\n- **背景杂乱** 物体可能会被混入背景，使得物体识别变得困难\n- **内部分类的广泛** 一些感兴趣的分类可能会很广泛，例如“椅子”这个分类，然而椅子的种类还有很多种，每一种的外观都不一样\n\n![All text](http://7xopvd.com1.z0.glb.clouddn.com/challenges.jpeg)\n\n一个优秀的好的图像分类模型必须克服这些多样性带来的挑战，与此同时还要保持对内部分类的敏感性。\n\n#### 数据驱动的研究方法\n到底该怎么写一个能对图像进行分类的算法呢？比起其他普遍的算法，比如为一个数组进行排序，图像分类算法并不是那么显而易见。所以，我们采取的方式并不是在代码里直接能体现分类这个动作，而是像给你的孩子一台电脑，里面存放了各种类别的图像样本，然后教你孩子观察和学习这些图像样本，并对他们进行分类。当孩子学习一段时间过后，给他一幅图像，如果学习效果显著，那么他之后就能对新来的图像进行分类了。\n\n上面这样的方法也被称为**数据驱动的研究方法**，这样的方法很依赖被标注好分类的训练**数据集**积累，下图展示了数据集的大概面貌：\n![Alt text](http://7xopvd.com1.z0.glb.clouddn.com/trainset.jpg)\n\n**数据分类的流程**\n从之前的介绍我们可以看出，图像分类所要做的事情便是将像素点输入，最后为其贴上一张分类标签，大致的流程可以用下面几项来描述：\n- **输入： **输入包含了N张图像，每一张带有一个正确的分类的标签，总共有K个不同的分类，这样的数据便称为**训练集**\n- **学习：**我们的任务是利用这样的训练集去学习每一个样本属于哪个分类，这个过程也被称为**训练分类器**或者**学习模型**\n- **评估： **最后，我们通过输入一个分类器从没接收过的新的图片集，然后验证此时的分类结果来验证分类器的可靠性和准确性。验证的手段一般是将分类器的预测结果和图像本身的正确分类进行比较，直观上看，我们希望看到的是预测大多结果都与正确的客观结果相符。\n\n#### 最近邻近分类器\n下面以**最近邻近分类器**作为数据驱动研究方法的第一个例子。虽然它和卷积神经网络（CNN）并不相关，而且在实际应用中几乎不被使用，但是通过它我们可以获取一些关于图像分类问题的基本实现方法。\n\n分类器采用的数据集：[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html)\n\nCIFAR-10里面包含了60000张32x32图片，每一张图片都被标注了类别，例如飞机、汽车、鸟等共计10个分类。这6万张图片中，有50000张是训练集，10000张是测试集。在下面这幅示意图中，你可以看到来自于10个分类的，每个分类下10张随机图片：\n![Alt text](http://7xopvd.com1.z0.glb.clouddn.com/nn.jpg)\n\n最近邻近分类器是这样工作的，它从测试集选取了一张图片，并与训练集中的每一张去比较，然后预测和它最相近的训练图片的分类。从上面的右图可以看出，对于左侧的每一张测试图片，右边都给出的10张和它最相似的图片。但是需要注意的是，找出的这10张图片的分类并不一定和预期的相符，一般而言只有3张，例如第8行的马头，匹配得到的第一张红色的汽车则不是马这个分类。误匹配的原因可能是红车的图片背景也是黑色的。\n\n具体而言，比较的过程是基于像素级的，对于测试图片和训练图片而言，每张图片的像素个数是：32x32x3，将两幅图像的像素数据进行相减，结果取绝对值，得到差值图像，然后将插值图像里的像素数据再相加，作为衡量两幅图像相似程度的依据，过程如下图所示：\n\n![All text](http://7xopvd.com1.z0.glb.clouddn.com/nneg.jpeg)\n\n如果将两幅图像的像素数据分别用两个一维向量$(I_1,I_2)$表示的话，它们间的差距可以用$L_1$距离来表示：$$L_1=|I_1-I_2|$$\n\n下面是最近邻近分类器的代码实现：\n**1. **载入CIFAR-10的数据，4个一维数组：训练图片、标签、测试图片、标签，$X_tr$代表的是来自训练样本的50000x32x32x3个像素点数据，$Y_tr$代表的是50000个训练样本的固有分类，范围是0-9：\n\n```python\nXtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide\n# flatten out all images to be one-dimensional\nXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072\nXte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072\n```\n\n**2. **有了数据，开始训练和评估我们的分类器：\n\n```python\nnn = NearestNeighbor() # create a Nearest Neighbor classifier class\nnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels\nYte_predict = nn.predict(Xte_rows) # predict labels on the test images\n# and now print the classification accuracy, which is the average number\n# of examples that are correctly predicted (i.e. label matches)\nprint 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )\n```\n\n精度这个指标一般用来衡量评估结果的好坏，它表示的是正确预测结果的百分比。\n\n$trrain(X,y)$函数两个参数：$X$是训练样本，$y$是样本的固有标签\n\n$predict(X))$函数一个参数：$X$是测试样本，返回的是测试结果标签\n\n分类器的用法介绍完了，下面是分类器实现的代码以及$L_1$距离的定义：\n\n```python\nimport numpy as np\n\nclass NearestNeighbor(object):\n  def __init__(self):\n    pass\n\n  def train(self, X, y):\n    \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n    # the nearest neighbor classifier simply remembers all the training data\n    self.Xtr = X\n    self.ytr = y\n\n  def predict(self, X):\n    \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n    num_test = X.shape[0]\n    # lets make sure that the output type matches the input type\n    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n\n    # loop over all test rows\n    for i in xrange(num_test):\n      # find the nearest training image to the i'th test image\n      # using the L1 distance (sum of absolute value differences)\n      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n      min_index = np.argmin(distances) # get the index with smallest distance\n      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n\n    return Ypred\n```\n\n用这个分类器跑CIFAR-10的数据，大概能达到38.6%的准确度，而随机猜测大概只能有10/100-10%的准确度。对于神经网络而言，大概能到95%的准确度，能与人脑相媲美\n\n对于距离计算选择，还有很多。\n\n（Lucien：第一天的学习总是告一段落了，第一章还是介绍了一些基本概念，读起来还是很轻松的，晚上还在实验室干了点活，perfect day！）","source":"_posts/standford-image-cnn-lesson-1.md","raw":"---\ntitle: 斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法\n---\n<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=429482207&auto=0&height=66\"></iframe>\n\n### 斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法\n\n#### 图像分类\n**概念** \n将图像作为输入，然后从固定的分类集合中选择一个标签为其贴上，一般而言，一些突出的计算机视觉任务，例如物体检测、图像分割都可以转化成为图像分类任务。\n\n<!--more-->\n\n**举个栗子** \n现在我有一幅图片，然后准备了4中分类标签：猫、狗、帽子和杯子。如下图所示，这张带有猫的图片尺寸是400x248，带有RGB三通道，每个通道位宽是8位，所以这幅图片的原生数据个数是：400x248x3=297600，每个数据范围是0-255，图像分类的任务便是把这么多的数据输入，然后最终输出“猫”这个分类。\n\n![All text](http://7xopvd.com1.z0.glb.clouddn.com/classify.png)\n\n**挑战**\n虽然在这样一幅图中寻找一只猫这样的任务对人类而言很简单，但在技术那几视觉算法领域，其中包含的一些挑战和困难是值得去考虑的。下面罗列了一些存在的问题，需要记住的是，原生图像的表示是三维的，即RGB：\n- **视角变化** 同样的物体，由于相机位置的不同，造成了物体在图像中的方位不同。\n- **伸缩变化** 物体在图像中的尺寸可能和现实世界中分类好的物体尺寸不同\n- **形变** 许多物体的形状并不是刚性的，可能会有很极端的形变方式\n- **光照条件** 光照强度在像素级上的影响是很严重的，例如光照太强引起的过饱和\n- **背景杂乱** 物体可能会被混入背景，使得物体识别变得困难\n- **内部分类的广泛** 一些感兴趣的分类可能会很广泛，例如“椅子”这个分类，然而椅子的种类还有很多种，每一种的外观都不一样\n\n![All text](http://7xopvd.com1.z0.glb.clouddn.com/challenges.jpeg)\n\n一个优秀的好的图像分类模型必须克服这些多样性带来的挑战，与此同时还要保持对内部分类的敏感性。\n\n#### 数据驱动的研究方法\n到底该怎么写一个能对图像进行分类的算法呢？比起其他普遍的算法，比如为一个数组进行排序，图像分类算法并不是那么显而易见。所以，我们采取的方式并不是在代码里直接能体现分类这个动作，而是像给你的孩子一台电脑，里面存放了各种类别的图像样本，然后教你孩子观察和学习这些图像样本，并对他们进行分类。当孩子学习一段时间过后，给他一幅图像，如果学习效果显著，那么他之后就能对新来的图像进行分类了。\n\n上面这样的方法也被称为**数据驱动的研究方法**，这样的方法很依赖被标注好分类的训练**数据集**积累，下图展示了数据集的大概面貌：\n![Alt text](http://7xopvd.com1.z0.glb.clouddn.com/trainset.jpg)\n\n**数据分类的流程**\n从之前的介绍我们可以看出，图像分类所要做的事情便是将像素点输入，最后为其贴上一张分类标签，大致的流程可以用下面几项来描述：\n- **输入： **输入包含了N张图像，每一张带有一个正确的分类的标签，总共有K个不同的分类，这样的数据便称为**训练集**\n- **学习：**我们的任务是利用这样的训练集去学习每一个样本属于哪个分类，这个过程也被称为**训练分类器**或者**学习模型**\n- **评估： **最后，我们通过输入一个分类器从没接收过的新的图片集，然后验证此时的分类结果来验证分类器的可靠性和准确性。验证的手段一般是将分类器的预测结果和图像本身的正确分类进行比较，直观上看，我们希望看到的是预测大多结果都与正确的客观结果相符。\n\n#### 最近邻近分类器\n下面以**最近邻近分类器**作为数据驱动研究方法的第一个例子。虽然它和卷积神经网络（CNN）并不相关，而且在实际应用中几乎不被使用，但是通过它我们可以获取一些关于图像分类问题的基本实现方法。\n\n分类器采用的数据集：[CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html)\n\nCIFAR-10里面包含了60000张32x32图片，每一张图片都被标注了类别，例如飞机、汽车、鸟等共计10个分类。这6万张图片中，有50000张是训练集，10000张是测试集。在下面这幅示意图中，你可以看到来自于10个分类的，每个分类下10张随机图片：\n![Alt text](http://7xopvd.com1.z0.glb.clouddn.com/nn.jpg)\n\n最近邻近分类器是这样工作的，它从测试集选取了一张图片，并与训练集中的每一张去比较，然后预测和它最相近的训练图片的分类。从上面的右图可以看出，对于左侧的每一张测试图片，右边都给出的10张和它最相似的图片。但是需要注意的是，找出的这10张图片的分类并不一定和预期的相符，一般而言只有3张，例如第8行的马头，匹配得到的第一张红色的汽车则不是马这个分类。误匹配的原因可能是红车的图片背景也是黑色的。\n\n具体而言，比较的过程是基于像素级的，对于测试图片和训练图片而言，每张图片的像素个数是：32x32x3，将两幅图像的像素数据进行相减，结果取绝对值，得到差值图像，然后将插值图像里的像素数据再相加，作为衡量两幅图像相似程度的依据，过程如下图所示：\n\n![All text](http://7xopvd.com1.z0.glb.clouddn.com/nneg.jpeg)\n\n如果将两幅图像的像素数据分别用两个一维向量$(I_1,I_2)$表示的话，它们间的差距可以用$L_1$距离来表示：$$L_1=|I_1-I_2|$$\n\n下面是最近邻近分类器的代码实现：\n**1. **载入CIFAR-10的数据，4个一维数组：训练图片、标签、测试图片、标签，$X_tr$代表的是来自训练样本的50000x32x32x3个像素点数据，$Y_tr$代表的是50000个训练样本的固有分类，范围是0-9：\n\n```python\nXtr, Ytr, Xte, Yte = load_CIFAR10('data/cifar10/') # a magic function we provide\n# flatten out all images to be one-dimensional\nXtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072\nXte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072\n```\n\n**2. **有了数据，开始训练和评估我们的分类器：\n\n```python\nnn = NearestNeighbor() # create a Nearest Neighbor classifier class\nnn.train(Xtr_rows, Ytr) # train the classifier on the training images and labels\nYte_predict = nn.predict(Xte_rows) # predict labels on the test images\n# and now print the classification accuracy, which is the average number\n# of examples that are correctly predicted (i.e. label matches)\nprint 'accuracy: %f' % ( np.mean(Yte_predict == Yte) )\n```\n\n精度这个指标一般用来衡量评估结果的好坏，它表示的是正确预测结果的百分比。\n\n$trrain(X,y)$函数两个参数：$X$是训练样本，$y$是样本的固有标签\n\n$predict(X))$函数一个参数：$X$是测试样本，返回的是测试结果标签\n\n分类器的用法介绍完了，下面是分类器实现的代码以及$L_1$距离的定义：\n\n```python\nimport numpy as np\n\nclass NearestNeighbor(object):\n  def __init__(self):\n    pass\n\n  def train(self, X, y):\n    \"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"\n    # the nearest neighbor classifier simply remembers all the training data\n    self.Xtr = X\n    self.ytr = y\n\n  def predict(self, X):\n    \"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"\n    num_test = X.shape[0]\n    # lets make sure that the output type matches the input type\n    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)\n\n    # loop over all test rows\n    for i in xrange(num_test):\n      # find the nearest training image to the i'th test image\n      # using the L1 distance (sum of absolute value differences)\n      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = 1)\n      min_index = np.argmin(distances) # get the index with smallest distance\n      Ypred[i] = self.ytr[min_index] # predict the label of the nearest example\n\n    return Ypred\n```\n\n用这个分类器跑CIFAR-10的数据，大概能达到38.6%的准确度，而随机猜测大概只能有10/100-10%的准确度。对于神经网络而言，大概能到95%的准确度，能与人脑相媲美\n\n对于距离计算选择，还有很多。\n\n（Lucien：第一天的学习总是告一段落了，第一章还是介绍了一些基本概念，读起来还是很轻松的，晚上还在实验室干了点活，perfect day！）","slug":"standford-image-cnn-lesson-1","published":1,"date":"2016-12-06T15:29:48.013Z","updated":"2016-12-07T08:48:46.717Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciwep1n330002qsfjol2jhy8a","content":"<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=\"330\" height=\"86\" src=\"//music.163.com/outchain/player?type=2&id=429482207&auto=0&height=66\"></iframe>\n\n<h3 id=\"斯坦福机器学习系列课程（一）-图像分类：数据驱动的研究方法\"><a href=\"#斯坦福机器学习系列课程（一）-图像分类：数据驱动的研究方法\" class=\"headerlink\" title=\"斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法\"></a>斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法</h3><h4 id=\"图像分类\"><a href=\"#图像分类\" class=\"headerlink\" title=\"图像分类\"></a>图像分类</h4><p><strong>概念</strong><br>将图像作为输入，然后从固定的分类集合中选择一个标签为其贴上，一般而言，一些突出的计算机视觉任务，例如物体检测、图像分割都可以转化成为图像分类任务。</p>\n<a id=\"more\"></a>\n<p><strong>举个栗子</strong><br>现在我有一幅图片，然后准备了4中分类标签：猫、狗、帽子和杯子。如下图所示，这张带有猫的图片尺寸是400x248，带有RGB三通道，每个通道位宽是8位，所以这幅图片的原生数据个数是：400x248x3=297600，每个数据范围是0-255，图像分类的任务便是把这么多的数据输入，然后最终输出“猫”这个分类。</p>\n<p><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/classify.png\" alt=\"All text\"></p>\n<p><strong>挑战</strong><br>虽然在这样一幅图中寻找一只猫这样的任务对人类而言很简单，但在技术那几视觉算法领域，其中包含的一些挑战和困难是值得去考虑的。下面罗列了一些存在的问题，需要记住的是，原生图像的表示是三维的，即RGB：</p>\n<ul>\n<li><strong>视角变化</strong> 同样的物体，由于相机位置的不同，造成了物体在图像中的方位不同。</li>\n<li><strong>伸缩变化</strong> 物体在图像中的尺寸可能和现实世界中分类好的物体尺寸不同</li>\n<li><strong>形变</strong> 许多物体的形状并不是刚性的，可能会有很极端的形变方式</li>\n<li><strong>光照条件</strong> 光照强度在像素级上的影响是很严重的，例如光照太强引起的过饱和</li>\n<li><strong>背景杂乱</strong> 物体可能会被混入背景，使得物体识别变得困难</li>\n<li><strong>内部分类的广泛</strong> 一些感兴趣的分类可能会很广泛，例如“椅子”这个分类，然而椅子的种类还有很多种，每一种的外观都不一样</li>\n</ul>\n<p><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/challenges.jpeg\" alt=\"All text\"></p>\n<p>一个优秀的好的图像分类模型必须克服这些多样性带来的挑战，与此同时还要保持对内部分类的敏感性。</p>\n<h4 id=\"数据驱动的研究方法\"><a href=\"#数据驱动的研究方法\" class=\"headerlink\" title=\"数据驱动的研究方法\"></a>数据驱动的研究方法</h4><p>到底该怎么写一个能对图像进行分类的算法呢？比起其他普遍的算法，比如为一个数组进行排序，图像分类算法并不是那么显而易见。所以，我们采取的方式并不是在代码里直接能体现分类这个动作，而是像给你的孩子一台电脑，里面存放了各种类别的图像样本，然后教你孩子观察和学习这些图像样本，并对他们进行分类。当孩子学习一段时间过后，给他一幅图像，如果学习效果显著，那么他之后就能对新来的图像进行分类了。</p>\n<p>上面这样的方法也被称为<strong>数据驱动的研究方法</strong>，这样的方法很依赖被标注好分类的训练<strong>数据集</strong>积累，下图展示了数据集的大概面貌：<br><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/trainset.jpg\" alt=\"Alt text\"></p>\n<p><strong>数据分类的流程</strong><br>从之前的介绍我们可以看出，图像分类所要做的事情便是将像素点输入，最后为其贴上一张分类标签，大致的流程可以用下面几项来描述：</p>\n<ul>\n<li><strong>输入： </strong>输入包含了N张图像，每一张带有一个正确的分类的标签，总共有K个不同的分类，这样的数据便称为<strong>训练集</strong></li>\n<li><strong>学习：</strong>我们的任务是利用这样的训练集去学习每一个样本属于哪个分类，这个过程也被称为<strong>训练分类器</strong>或者<strong>学习模型</strong></li>\n<li><strong>评估： </strong>最后，我们通过输入一个分类器从没接收过的新的图片集，然后验证此时的分类结果来验证分类器的可靠性和准确性。验证的手段一般是将分类器的预测结果和图像本身的正确分类进行比较，直观上看，我们希望看到的是预测大多结果都与正确的客观结果相符。</li>\n</ul>\n<h4 id=\"最近邻近分类器\"><a href=\"#最近邻近分类器\" class=\"headerlink\" title=\"最近邻近分类器\"></a>最近邻近分类器</h4><p>下面以<strong>最近邻近分类器</strong>作为数据驱动研究方法的第一个例子。虽然它和卷积神经网络（CNN）并不相关，而且在实际应用中几乎不被使用，但是通过它我们可以获取一些关于图像分类问题的基本实现方法。</p>\n<p>分类器采用的数据集：<a href=\"http://www.cs.toronto.edu/~kriz/cifar.html\" target=\"_blank\" rel=\"external\">CIFAR-10</a></p>\n<p>CIFAR-10里面包含了60000张32x32图片，每一张图片都被标注了类别，例如飞机、汽车、鸟等共计10个分类。这6万张图片中，有50000张是训练集，10000张是测试集。在下面这幅示意图中，你可以看到来自于10个分类的，每个分类下10张随机图片：<br><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/nn.jpg\" alt=\"Alt text\"></p>\n<p>最近邻近分类器是这样工作的，它从测试集选取了一张图片，并与训练集中的每一张去比较，然后预测和它最相近的训练图片的分类。从上面的右图可以看出，对于左侧的每一张测试图片，右边都给出的10张和它最相似的图片。但是需要注意的是，找出的这10张图片的分类并不一定和预期的相符，一般而言只有3张，例如第8行的马头，匹配得到的第一张红色的汽车则不是马这个分类。误匹配的原因可能是红车的图片背景也是黑色的。</p>\n<p>具体而言，比较的过程是基于像素级的，对于测试图片和训练图片而言，每张图片的像素个数是：32x32x3，将两幅图像的像素数据进行相减，结果取绝对值，得到差值图像，然后将插值图像里的像素数据再相加，作为衡量两幅图像相似程度的依据，过程如下图所示：</p>\n<p><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/nneg.jpeg\" alt=\"All text\"></p>\n<p>如果将两幅图像的像素数据分别用两个一维向量$(I_1,I_2)$表示的话，它们间的差距可以用$L_1$距离来表示：$$L_1=|I_1-I_2|$$</p>\n<p>下面是最近邻近分类器的代码实现：<br><strong>1. </strong>载入CIFAR-10的数据，4个一维数组：训练图片、标签、测试图片、标签，$X_tr$代表的是来自训练样本的50000x32x32x3个像素点数据，$Y_tr$代表的是50000个训练样本的固有分类，范围是0-9：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class=\"string\">'data/cifar10/'</span>) <span class=\"comment\"># a magic function we provide</span></div><div class=\"line\"><span class=\"comment\"># flatten out all images to be one-dimensional</span></div><div class=\"line\">Xtr_rows = Xtr.reshape(Xtr.shape[<span class=\"number\">0</span>], <span class=\"number\">32</span> * <span class=\"number\">32</span> * <span class=\"number\">3</span>) <span class=\"comment\"># Xtr_rows becomes 50000 x 3072</span></div><div class=\"line\">Xte_rows = Xte.reshape(Xte.shape[<span class=\"number\">0</span>], <span class=\"number\">32</span> * <span class=\"number\">32</span> * <span class=\"number\">3</span>) <span class=\"comment\"># Xte_rows becomes 10000 x 3072</span></div></pre></td></tr></table></figure>\n<p><strong>2. </strong>有了数据，开始训练和评估我们的分类器：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">nn = NearestNeighbor() <span class=\"comment\"># create a Nearest Neighbor classifier class</span></div><div class=\"line\">nn.train(Xtr_rows, Ytr) <span class=\"comment\"># train the classifier on the training images and labels</span></div><div class=\"line\">Yte_predict = nn.predict(Xte_rows) <span class=\"comment\"># predict labels on the test images</span></div><div class=\"line\"><span class=\"comment\"># and now print the classification accuracy, which is the average number</span></div><div class=\"line\"><span class=\"comment\"># of examples that are correctly predicted (i.e. label matches)</span></div><div class=\"line\"><span class=\"keyword\">print</span> <span class=\"string\">'accuracy: %f'</span> % ( np.mean(Yte_predict == Yte) )</div></pre></td></tr></table></figure>\n<p>精度这个指标一般用来衡量评估结果的好坏，它表示的是正确预测结果的百分比。</p>\n<p>$trrain(X,y)$函数两个参数：$X$是训练样本，$y$是样本的固有标签</p>\n<p>$predict(X))$函数一个参数：$X$是测试样本，返回的是测试结果标签</p>\n<p>分类器的用法介绍完了，下面是分类器实现的代码以及$L_1$距离的定义：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NearestNeighbor</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">pass</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self, X, y)</span>:</span></div><div class=\"line\">    <span class=\"string\">\"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"</span></div><div class=\"line\">    <span class=\"comment\"># the nearest neighbor classifier simply remembers all the training data</span></div><div class=\"line\">    self.Xtr = X</div><div class=\"line\">    self.ytr = y</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, X)</span>:</span></div><div class=\"line\">    <span class=\"string\">\"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"</span></div><div class=\"line\">    num_test = X.shape[<span class=\"number\">0</span>]</div><div class=\"line\">    <span class=\"comment\"># lets make sure that the output type matches the input type</span></div><div class=\"line\">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># loop over all test rows</span></div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(num_test):</div><div class=\"line\">      <span class=\"comment\"># find the nearest training image to the i'th test image</span></div><div class=\"line\">      <span class=\"comment\"># using the L1 distance (sum of absolute value differences)</span></div><div class=\"line\">      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = <span class=\"number\">1</span>)</div><div class=\"line\">      min_index = np.argmin(distances) <span class=\"comment\"># get the index with smallest distance</span></div><div class=\"line\">      Ypred[i] = self.ytr[min_index] <span class=\"comment\"># predict the label of the nearest example</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">return</span> Ypred</div></pre></td></tr></table></figure>\n<p>用这个分类器跑CIFAR-10的数据，大概能达到38.6%的准确度，而随机猜测大概只能有10/100-10%的准确度。对于神经网络而言，大概能到95%的准确度，能与人脑相媲美</p>\n<p>对于距离计算选择，还有很多。</p>\n<p>（Lucien：第一天的学习总是告一段落了，第一章还是介绍了一些基本概念，读起来还是很轻松的，晚上还在实验室干了点活，perfect day！）</p>\n","excerpt":"<iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&id=429482207&auto=0&height=66\"></iframe>\n\n<h3 id=\"斯坦福机器学习系列课程（一）-图像分类：数据驱动的研究方法\"><a href=\"#斯坦福机器学习系列课程（一）-图像分类：数据驱动的研究方法\" class=\"headerlink\" title=\"斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法\"></a>斯坦福机器学习系列课程（一） 图像分类：数据驱动的研究方法</h3><h4 id=\"图像分类\"><a href=\"#图像分类\" class=\"headerlink\" title=\"图像分类\"></a>图像分类</h4><p><strong>概念</strong><br>将图像作为输入，然后从固定的分类集合中选择一个标签为其贴上，一般而言，一些突出的计算机视觉任务，例如物体检测、图像分割都可以转化成为图像分类任务。</p>","more":"<p><strong>举个栗子</strong><br>现在我有一幅图片，然后准备了4中分类标签：猫、狗、帽子和杯子。如下图所示，这张带有猫的图片尺寸是400x248，带有RGB三通道，每个通道位宽是8位，所以这幅图片的原生数据个数是：400x248x3=297600，每个数据范围是0-255，图像分类的任务便是把这么多的数据输入，然后最终输出“猫”这个分类。</p>\n<p><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/classify.png\" alt=\"All text\"></p>\n<p><strong>挑战</strong><br>虽然在这样一幅图中寻找一只猫这样的任务对人类而言很简单，但在技术那几视觉算法领域，其中包含的一些挑战和困难是值得去考虑的。下面罗列了一些存在的问题，需要记住的是，原生图像的表示是三维的，即RGB：</p>\n<ul>\n<li><strong>视角变化</strong> 同样的物体，由于相机位置的不同，造成了物体在图像中的方位不同。</li>\n<li><strong>伸缩变化</strong> 物体在图像中的尺寸可能和现实世界中分类好的物体尺寸不同</li>\n<li><strong>形变</strong> 许多物体的形状并不是刚性的，可能会有很极端的形变方式</li>\n<li><strong>光照条件</strong> 光照强度在像素级上的影响是很严重的，例如光照太强引起的过饱和</li>\n<li><strong>背景杂乱</strong> 物体可能会被混入背景，使得物体识别变得困难</li>\n<li><strong>内部分类的广泛</strong> 一些感兴趣的分类可能会很广泛，例如“椅子”这个分类，然而椅子的种类还有很多种，每一种的外观都不一样</li>\n</ul>\n<p><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/challenges.jpeg\" alt=\"All text\"></p>\n<p>一个优秀的好的图像分类模型必须克服这些多样性带来的挑战，与此同时还要保持对内部分类的敏感性。</p>\n<h4 id=\"数据驱动的研究方法\"><a href=\"#数据驱动的研究方法\" class=\"headerlink\" title=\"数据驱动的研究方法\"></a>数据驱动的研究方法</h4><p>到底该怎么写一个能对图像进行分类的算法呢？比起其他普遍的算法，比如为一个数组进行排序，图像分类算法并不是那么显而易见。所以，我们采取的方式并不是在代码里直接能体现分类这个动作，而是像给你的孩子一台电脑，里面存放了各种类别的图像样本，然后教你孩子观察和学习这些图像样本，并对他们进行分类。当孩子学习一段时间过后，给他一幅图像，如果学习效果显著，那么他之后就能对新来的图像进行分类了。</p>\n<p>上面这样的方法也被称为<strong>数据驱动的研究方法</strong>，这样的方法很依赖被标注好分类的训练<strong>数据集</strong>积累，下图展示了数据集的大概面貌：<br><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/trainset.jpg\" alt=\"Alt text\"></p>\n<p><strong>数据分类的流程</strong><br>从之前的介绍我们可以看出，图像分类所要做的事情便是将像素点输入，最后为其贴上一张分类标签，大致的流程可以用下面几项来描述：</p>\n<ul>\n<li><strong>输入： </strong>输入包含了N张图像，每一张带有一个正确的分类的标签，总共有K个不同的分类，这样的数据便称为<strong>训练集</strong></li>\n<li><strong>学习：</strong>我们的任务是利用这样的训练集去学习每一个样本属于哪个分类，这个过程也被称为<strong>训练分类器</strong>或者<strong>学习模型</strong></li>\n<li><strong>评估： </strong>最后，我们通过输入一个分类器从没接收过的新的图片集，然后验证此时的分类结果来验证分类器的可靠性和准确性。验证的手段一般是将分类器的预测结果和图像本身的正确分类进行比较，直观上看，我们希望看到的是预测大多结果都与正确的客观结果相符。</li>\n</ul>\n<h4 id=\"最近邻近分类器\"><a href=\"#最近邻近分类器\" class=\"headerlink\" title=\"最近邻近分类器\"></a>最近邻近分类器</h4><p>下面以<strong>最近邻近分类器</strong>作为数据驱动研究方法的第一个例子。虽然它和卷积神经网络（CNN）并不相关，而且在实际应用中几乎不被使用，但是通过它我们可以获取一些关于图像分类问题的基本实现方法。</p>\n<p>分类器采用的数据集：<a href=\"http://www.cs.toronto.edu/~kriz/cifar.html\">CIFAR-10</a></p>\n<p>CIFAR-10里面包含了60000张32x32图片，每一张图片都被标注了类别，例如飞机、汽车、鸟等共计10个分类。这6万张图片中，有50000张是训练集，10000张是测试集。在下面这幅示意图中，你可以看到来自于10个分类的，每个分类下10张随机图片：<br><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/nn.jpg\" alt=\"Alt text\"></p>\n<p>最近邻近分类器是这样工作的，它从测试集选取了一张图片，并与训练集中的每一张去比较，然后预测和它最相近的训练图片的分类。从上面的右图可以看出，对于左侧的每一张测试图片，右边都给出的10张和它最相似的图片。但是需要注意的是，找出的这10张图片的分类并不一定和预期的相符，一般而言只有3张，例如第8行的马头，匹配得到的第一张红色的汽车则不是马这个分类。误匹配的原因可能是红车的图片背景也是黑色的。</p>\n<p>具体而言，比较的过程是基于像素级的，对于测试图片和训练图片而言，每张图片的像素个数是：32x32x3，将两幅图像的像素数据进行相减，结果取绝对值，得到差值图像，然后将插值图像里的像素数据再相加，作为衡量两幅图像相似程度的依据，过程如下图所示：</p>\n<p><img src=\"http://7xopvd.com1.z0.glb.clouddn.com/nneg.jpeg\" alt=\"All text\"></p>\n<p>如果将两幅图像的像素数据分别用两个一维向量$(I_1,I_2)$表示的话，它们间的差距可以用$L_1$距离来表示：$$L_1=|I_1-I_2|$$</p>\n<p>下面是最近邻近分类器的代码实现：<br><strong>1. </strong>载入CIFAR-10的数据，4个一维数组：训练图片、标签、测试图片、标签，$X_tr$代表的是来自训练样本的50000x32x32x3个像素点数据，$Y_tr$代表的是50000个训练样本的固有分类，范围是0-9：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div></pre></td><td class=\"code\"><pre><div class=\"line\">Xtr, Ytr, Xte, Yte = load_CIFAR10(<span class=\"string\">'data/cifar10/'</span>) <span class=\"comment\"># a magic function we provide</span></div><div class=\"line\"><span class=\"comment\"># flatten out all images to be one-dimensional</span></div><div class=\"line\">Xtr_rows = Xtr.reshape(Xtr.shape[<span class=\"number\">0</span>], <span class=\"number\">32</span> * <span class=\"number\">32</span> * <span class=\"number\">3</span>) <span class=\"comment\"># Xtr_rows becomes 50000 x 3072</span></div><div class=\"line\">Xte_rows = Xte.reshape(Xte.shape[<span class=\"number\">0</span>], <span class=\"number\">32</span> * <span class=\"number\">32</span> * <span class=\"number\">3</span>) <span class=\"comment\"># Xte_rows becomes 10000 x 3072</span></div></pre></td></tr></table></figure>\n<p><strong>2. </strong>有了数据，开始训练和评估我们的分类器：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div></pre></td><td class=\"code\"><pre><div class=\"line\">nn = NearestNeighbor() <span class=\"comment\"># create a Nearest Neighbor classifier class</span></div><div class=\"line\">nn.train(Xtr_rows, Ytr) <span class=\"comment\"># train the classifier on the training images and labels</span></div><div class=\"line\">Yte_predict = nn.predict(Xte_rows) <span class=\"comment\"># predict labels on the test images</span></div><div class=\"line\"><span class=\"comment\"># and now print the classification accuracy, which is the average number</span></div><div class=\"line\"><span class=\"comment\"># of examples that are correctly predicted (i.e. label matches)</span></div><div class=\"line\"><span class=\"keyword\">print</span> <span class=\"string\">'accuracy: %f'</span> % ( np.mean(Yte_predict == Yte) )</div></pre></td></tr></table></figure>\n<p>精度这个指标一般用来衡量评估结果的好坏，它表示的是正确预测结果的百分比。</p>\n<p>$trrain(X,y)$函数两个参数：$X$是训练样本，$y$是样本的固有标签</p>\n<p>$predict(X))$函数一个参数：$X$是测试样本，返回的是测试结果标签</p>\n<p>分类器的用法介绍完了，下面是分类器实现的代码以及$L_1$距离的定义：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div><div class=\"line\">13</div><div class=\"line\">14</div><div class=\"line\">15</div><div class=\"line\">16</div><div class=\"line\">17</div><div class=\"line\">18</div><div class=\"line\">19</div><div class=\"line\">20</div><div class=\"line\">21</div><div class=\"line\">22</div><div class=\"line\">23</div><div class=\"line\">24</div><div class=\"line\">25</div><div class=\"line\">26</div><div class=\"line\">27</div></pre></td><td class=\"code\"><pre><div class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</div><div class=\"line\"></div><div class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NearestNeighbor</span><span class=\"params\">(object)</span>:</span></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self)</span>:</span></div><div class=\"line\">    <span class=\"keyword\">pass</span></div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self, X, y)</span>:</span></div><div class=\"line\">    <span class=\"string\">\"\"\" X is N x D where each row is an example. Y is 1-dimension of size N \"\"\"</span></div><div class=\"line\">    <span class=\"comment\"># the nearest neighbor classifier simply remembers all the training data</span></div><div class=\"line\">    self.Xtr = X</div><div class=\"line\">    self.ytr = y</div><div class=\"line\"></div><div class=\"line\">  <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self, X)</span>:</span></div><div class=\"line\">    <span class=\"string\">\"\"\" X is N x D where each row is an example we wish to predict label for \"\"\"</span></div><div class=\"line\">    num_test = X.shape[<span class=\"number\">0</span>]</div><div class=\"line\">    <span class=\"comment\"># lets make sure that the output type matches the input type</span></div><div class=\"line\">    Ypred = np.zeros(num_test, dtype = self.ytr.dtype)</div><div class=\"line\"></div><div class=\"line\">    <span class=\"comment\"># loop over all test rows</span></div><div class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> xrange(num_test):</div><div class=\"line\">      <span class=\"comment\"># find the nearest training image to the i'th test image</span></div><div class=\"line\">      <span class=\"comment\"># using the L1 distance (sum of absolute value differences)</span></div><div class=\"line\">      distances = np.sum(np.abs(self.Xtr - X[i,:]), axis = <span class=\"number\">1</span>)</div><div class=\"line\">      min_index = np.argmin(distances) <span class=\"comment\"># get the index with smallest distance</span></div><div class=\"line\">      Ypred[i] = self.ytr[min_index] <span class=\"comment\"># predict the label of the nearest example</span></div><div class=\"line\"></div><div class=\"line\">    <span class=\"keyword\">return</span> Ypred</div></pre></td></tr></table></figure>\n<p>用这个分类器跑CIFAR-10的数据，大概能达到38.6%的准确度，而随机猜测大概只能有10/100-10%的准确度。对于神经网络而言，大概能到95%的准确度，能与人脑相媲美</p>\n<p>对于距离计算选择，还有很多。</p>\n<p>（Lucien：第一天的学习总是告一段落了，第一章还是介绍了一些基本概念，读起来还是很轻松的，晚上还在实验室干了点活，perfect day！）</p>"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}